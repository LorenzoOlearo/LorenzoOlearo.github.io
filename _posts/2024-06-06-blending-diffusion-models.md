---
layout: post
title: Blending Diffusion Models
date: 2024-06-06 09:00:00
description: How to blend concepts with diffusion models
tags: diffusion_models, image_synthesis, blending
categories: research
thumbnail: assets/img/blog/2024-05-11-blending-diffusion-models/thumbnail.png
---

#### Let's say we have two different concepts, can we blend them together with diffusion models?

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    SWITCH blending "lion" with "cat", on the left the lion, on the right the cat, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise.
</div>

This is the main research question that we tried to answer in our latest work
that will be soon published. Short answer: yes, we can. Long answer: it's a bit
more complicated than that and there are many different ways to do it.

I won't go into the details of the paper here, but I will just show here some of
the most interesting results that we got. I recommend reading the paper if you
are interested in the methodology and the details of the experiments. 

Four different methods are explored in the paper, one of which, with the best of
our knowledge, is novel. The methods are:

1. **TEXTUAL**: the blend of two different concepts is the linear interpolation
   of their text embeddings. 
2. **SWITCH**: starts the denoising process with one concept and then switches,
   at some predefined point, to the other concept.
3. **UNET**: at each diffusion step, encodes in the U-Net the image latent using
   the first concept and then decodes it using the second one.
4. **ALTERNATE**: uses the first concept for the even diffusion steps and the second one
   for the odd steps.

Once again, for the detailed explanation, implementation and comparison of this
blending methods, please refer to the paper.

Each of the methods illustrated in the paper is implemented from scratch using the
Diffusers library, the full implementation is available in my GitHub repository 
[blending-diffusion-models](https://github.com/LorenzoOlearo/blending-diffusion-models)

The pipelines are implemented with UNet, text-encoder, tokenizer, and VAEs
weights from `CompVis/stable-diffusion-v1-4`. All of the samples shown here are
generated using the same `UniPCMultistepScheduler` scheduler at 25 steps.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    UNET blending of "Fish" with "Tiger", on the left the fish, on the right the tiger, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise.
</div>


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    TEXTUAL blending "Frog" with "Lizard", on the left the frog, on the right the lizard, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise.
</div>


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    UNET blending "penguin" with "lemon", on the left the penguin, on the right the lemon, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise.
</div>


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk lamp-BLEND-Construction Crane.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption">
    ALTERNATE blending "Desk lamp" with "Construction Crane"
</div>