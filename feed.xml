<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://lorenzoolearo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lorenzoolearo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-07T20:09:21+00:00</updated><id>https://lorenzoolearo.github.io/feed.xml</id><title type="html">blank</title><subtitle>The academic website of Lorenzo Olearo. </subtitle><entry><title type="html">Blending Diffusion Models</title><link href="https://lorenzoolearo.github.io/blog/2024/blending-diffusion-models/" rel="alternate" type="text/html" title="Blending Diffusion Models"/><published>2024-06-06T09:00:00+00:00</published><updated>2024-06-06T09:00:00+00:00</updated><id>https://lorenzoolearo.github.io/blog/2024/blending-diffusion-models</id><content type="html" xml:base="https://lorenzoolearo.github.io/blog/2024/blending-diffusion-models/"><![CDATA[<h4 id="lets-say-we-have-two-different-concepts-can-we-blend-them-together-with-diffusion-models">Let’s say we have two different concepts, can we blend them together with diffusion models?</h4> <p>This is the main research question that we tried to answer in our latest work that will be soon published. Short answer: yes, we can. Long answer: it’s a bit more complicated than that and there are many different ways to do it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SWITCH blending "lion" with "cat", on the left the lion, on the right the cat, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise. </div> <p>I won’t go into the details of the paper here, but I will just show here some of the most interesting results that we got. I recommend reading the paper if you are interested in the methodology and the details of the experiments.</p> <p>Four different methods are explored in the paper, one of which, with the best of our knowledge, is novel. The methods are:</p> <ol> <li><strong>TEXTUAL</strong>: the blend of two different concepts is the linear interpolation of their text embeddings.</li> <li><strong>SWITCH</strong>: starts the denoising process with one concept and then switches, at some predefined point, to the other concept.</li> <li><strong>UNET</strong>: at each diffusion step, encodes in the U-Net the image latent using the first concept and then decodes it using the second one.</li> <li><strong>ALTERNATE</strong>: uses the first concept for the even diffusion steps and the second one for the odd steps.</li> </ol> <p>Once again, for the detailed explanation, implementation and comparison of this blending methods, please refer to the paper.</p> <h4 id="experimental-setup">Experimental setup</h4> <p>Each of the methods illustrated in the paper is implemented from scratch using the Diffusers library, the full implementation is available in my GitHub repository <a href="https://github.com/LorenzoOlearo/blending-diffusion-models">blending-diffusion-models</a></p> <p>The pipelines are implemented with UNet, text-encoder, tokenizer, and VAEs weights from <code class="language-plaintext highlighter-rouge">CompVis/stable-diffusion-v1-4</code>. All of the samples shown here are generated using the same <code class="language-plaintext highlighter-rouge">UniPCMultistepScheduler</code> scheduler at 25 steps.</p> <blockquote> <p>In the following figures, the image on the left is the first prompt, the image on the right is the second prompt, and the image in the middle is the blend of the two with the method indicated in the caption and title.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> UNET blending of "Fish" with "Tiger", on the left the fish, on the right the tiger, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> TEXTUAL blending "Frog" with "Lizard" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> UNET blending "penguin" with "lemon" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ALTERNATE blending "Desk lamp" with "Construction Crane" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SWITCH blending "Motherboard" with "Aerial view of a city" </div> <p>With this post I wanted to give a just brief overview of the work that we have done with our latest submission. As soon as the paper is published I will update this post with the link to it.</p> <p>This research is part of my Master’s thesis on understanding the spatial behavior of Diffusion Models. If you are interested in this topic and would like to collaborate feel free to contact me through any of the contacts at the bottom of this page.</p> <p>I will update the post with the link to the paper as soon as it is published. Stay tuned!</p>]]></content><author><name></name></author><category term="research"/><category term="diffusion_models,"/><category term="image_synthesis,"/><category term="blending"/><summary type="html"><![CDATA[How to blend concepts with diffusion models]]></summary></entry></feed>