<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="lorenzo.olearo.com/feed.xml" rel="self" type="application/atom+xml"/><link href="lorenzo.olearo.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-04T08:42:05+00:00</updated><id>lorenzo.olearo.com/feed.xml</id><title type="html">blank</title><subtitle>The academic website of Lorenzo Olearo. </subtitle><entry><title type="html">Blending Diffusion Models</title><link href="lorenzo.olearo.com/blog/2024/blending-diffusion-models/" rel="alternate" type="text/html" title="Blending Diffusion Models"/><published>2024-06-06T09:00:00+00:00</published><updated>2024-06-06T09:00:00+00:00</updated><id>lorenzo.olearo.com/blog/2024/blending-diffusion-models</id><content type="html" xml:base="lorenzo.olearo.com/blog/2024/blending-diffusion-models/"><![CDATA[<h4 id="assume-two-different-concepts-are-given-can-we-blend-them-together-with-diffusion-models">Assume two different concepts are given, can we blend them together with diffusion models?</h4> <p>This is the main research question that we tried to answer in our latest work that will be soon published. Short answer: yes, we can. Long answer: it’s a bit more complicated than that and there are many different ways to do it.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/blending-lion-BLEND-cat.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SWITCH blending "lion" with "cat", on the left the lion, on the right the cat, and in the middle their blend. Note that each image is generated by the same exact diffusion pipeline using the same exact initial noise. </div> <p>The paper is currently in press and will be presented at the end of this month, this post is meant to give a brief overview of the work that we have done and the results that we have obtained. I will therefore not go into methodological details, if you are interested feel free to contact me or refer to the paper once it is published. I will update this post with the link to the paper as soon as it is publicly available.</p> <p>Four different methods are explored in our research, one of which, to the best of our knowledge, is novel. The methods are:</p> <ol> <li><strong>TEXTUAL</strong>: the blend of two different concepts is the linear interpolation of their text embeddings.</li> <li><strong>SWITCH</strong>: starts the denoising process with one concept and then switches, at some predefined point, to the other concept.</li> <li><strong>UNET</strong>: at each diffusion step, encode in the U-Net the image latent using the first concept and then decode it using the second one.</li> <li><strong>ALTERNATE</strong>: uses the first concept for the even diffusion steps and the second one for the odd steps.</li> </ol> <p>Once again, for a detailed explanation, implementation and comparison of this blending method, please refer to the paper.</p> <h4 id="experimental-setup">Experimental setup</h4> <p>Each of the methods illustrated in the paper is implemented from scratch using the Diffusers library, the full implementation is available in my GitHub repository <a href="https://github.com/LorenzoOlearo/blending-diffusion-models">blending-diffusion-models</a></p> <p>The pipelines are implemented with UNet, text-encoder, tokenizer, and VAEs weights from <code class="language-plaintext highlighter-rouge">CompVis/stable-diffusion-v1-4</code>. All of the samples shown here are generated using the same <code class="language-plaintext highlighter-rouge">UniPCMultistepScheduler</code> scheduler at 25 steps.</p> <blockquote> <p>In the following figures, the image on the left is the first prompt, the image on the right is the second prompt, and the image in the middle is the blend of the two with the method indicated in the caption and title.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Fish-BLEND-Tiger.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> UNET blending of "Fish" with "Tiger" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Frog-BLEND-Lizard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> TEXTUAL blending "Frog" with "Lizard" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Penguin-BLEND-Lemon.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> UNET blending "penguin" with "lemon" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Desk%20lamp-BLEND-Construction%20Crane.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ALTERNATE blending "Desk lamp" with "Construction Crane" </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-480.webp 480w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-800.webp 800w,/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city-1400.webp 1400w," sizes="195vw" type="image/webp"/> <img src="/assets/img/blog/2024-05-11-blending-diffusion-models/comparison-Motherboard-BLEND-Aerial%20view%20of%20a%20city.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> SWITCH blending "Motherboard" with "Aerial view of a city" </div> <h4 id="conclusions">Conclusions</h4> <p>The results of this research show that diffusion models can blend different concepts, however, from our findings, there is no definitive method to do so. Each of them exhibits different behaviors and effects as results vary depending on the semantical distance between the two concepts, as well as the spatial similarity of the two images synthesized from the respective prompts.</p> <p>This research is part of my Master’s thesis on understanding the spatial behavior of Diffusion Models. If you are interested in this topic and would like to collaborate feel free to contact me through any of the contacts at the bottom of this page. I will update the post with the link to the paper as soon as it is published. Stay tuned!</p>]]></content><author><name></name></author><category term="research"/><category term="diffusion_models,"/><category term="image_synthesis,"/><category term="blending"/><summary type="html"><![CDATA[How to blend concepts with diffusion models]]></summary></entry></feed>